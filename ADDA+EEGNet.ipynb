{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Activation, Permute, Dropout\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from tensorflow.keras.layers import SeparableConv2D, DepthwiseConv2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import SpatialDropout2D\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras.layers import Input, Flatten\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def EEGNet(nb_classes, Chans = 64, Samples = 128, \n",
    "             dropoutRate = 0.5, kernLength = 64, F1 = 8, \n",
    "             D = 2, F2 = 16, norm_rate = 0.25, dropoutType = 'Dropout'):\n",
    "    \"\"\" Keras Implementation of EEGNet\n",
    "    http://iopscience.iop.org/article/10.1088/1741-2552/aace8c/meta\n",
    "\n",
    "    Inputs:\n",
    "        \n",
    "      nb_classes      : int, number of classes to classify\n",
    "      Chans, Samples  : number of channels and time points in the EEG data\n",
    "      dropoutRate     : dropout fraction\n",
    "      kernLength      : length of temporal convolution in first layer. We found\n",
    "                        that setting this to be half the sampling rate worked\n",
    "                        well in practice. For the SMR dataset in particular\n",
    "                        since the data was high-passed at 4Hz we used a kernel\n",
    "                        length of 32.     \n",
    "      F1, F2          : number of temporal filters (F1) and number of pointwise\n",
    "                        filters (F2) to learn. Default: F1 = 8, F2 = F1 * D. \n",
    "      D               : number of spatial filters to learn within each temporal\n",
    "                        convolution. Default: D = 2\n",
    "      dropoutType     : Either SpatialDropout2D or Dropout, passed as a string.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    if dropoutType == 'SpatialDropout2D':\n",
    "        dropoutType = SpatialDropout2D\n",
    "    elif dropoutType == 'Dropout':\n",
    "        dropoutType = Dropout\n",
    "    else:\n",
    "        raise ValueError('dropoutType must be one of SpatialDropout2D '\n",
    "                         'or Dropout, passed as a string.')\n",
    "    \n",
    "    input1   = Input(shape = (1, Chans, Samples))\n",
    "\n",
    "    ##################################################################\n",
    "    block1       = Conv2D(F1, (1, kernLength), padding = 'same',\n",
    "                                   input_shape = (1, Chans, Samples),\n",
    "                                   use_bias = False)(input1)\n",
    "    block1       = BatchNormalization(axis = 1)(block1)\n",
    "    block1       = DepthwiseConv2D((Chans, 1), use_bias = False, \n",
    "                                   depth_multiplier = D,\n",
    "                                   depthwise_constraint = max_norm(1.))(block1)\n",
    "    block1       = BatchNormalization(axis = 1)(block1)\n",
    "    block1       = Activation('elu', name = 'elu_1')(block1)\n",
    "    block1       = AveragePooling2D((1, 4))(block1)\n",
    "    block1       = dropoutType(dropoutRate)(block1)\n",
    "    \n",
    "    block2       = SeparableConv2D(F2, (1, 16),\n",
    "                                   use_bias = False, padding = 'same')(block1)\n",
    "    block2       = BatchNormalization(axis = 1)(block2)\n",
    "    block2       = Activation('elu', name = 'elu_2')(block2)\n",
    "    block2       = AveragePooling2D((1, 8))(block2)\n",
    "    block2       = dropoutType(dropoutRate)(block2)\n",
    "        \n",
    "    flatten      = Flatten(name = 'flatten')(block2)\n",
    "    \n",
    "    dense        = Dense(nb_classes, name = 'dense', \n",
    "                         kernel_constraint = max_norm(norm_rate))(flatten)\n",
    "    softmax      = Activation('softmax', name = 'softmax')(dense)\n",
    "\n",
    "        \n",
    "    return Model(inputs=input1, outputs=softmax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(input_tensor):\n",
    "    dense1 = Dense(400, activation = 'relu')(input_tensor)\n",
    "    dense1 = Dense(100, activation = 'relu')(dense1)\n",
    "    dense1 = Dense(1, activation = 'sigmoid')(dense1)\n",
    "    model = Model(inputs = input_tensor, output = dense1)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(X_train, y_train, batch_size):\n",
    "    idx = 0\n",
    "    total = len(X_train)\n",
    "    while 1:\n",
    "        for i in range(total/batch_size):\n",
    "            p = np.random.permutation(len(X_train)) # shuffle each time \n",
    "            X_train = X_train[p]\n",
    "            y_train = y_train[p]\n",
    "            yield X_train[i*batch_size:(i+1)*batch_size], y_train[i*batch_size:(i+1)*batch_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(nb_classes, chans, samples):\n",
    "    \n",
    "#define source EEGNet and target EEGNet\n",
    "\n",
    "    source_model = EEGNet(nb_classes=2, Chans=chans, Samples=samples,\n",
    "                                           dropoutRate=0.65, kernLength=32, F1=8, D=2, F2=16,\n",
    "                                           dropoutType='Dropout')\n",
    "    source_model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    \n",
    "    source_feature_model = Model(inputs = target_model.input, \n",
    "                                  outputs = target_model.get_layer('elu_2').output)\n",
    "    \n",
    "    feature_tensor = Input(shape = target_model.get_layer('elu_2').shape)\n",
    "    \n",
    "    discriminator_model = build_discriminator(feature_tensor)\n",
    "    discriminator_model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    \n",
    "    target_model = EEGNet(nb_classes=2, Chans=chans, Samples=samples,\n",
    "                                           dropoutRate=0.65, kernLength=32, F1=8, D=2, F2=16,\n",
    "                                           dropoutType='Dropout')\n",
    "    \n",
    "    target_feature_model = Model(inputs = target_model.input, \n",
    "                                  outputs = target_model.get_layer('elu_2').output)\n",
    "    \n",
    "    temp = target_feature_model(target_model.input)\n",
    "    \n",
    "    combined_model = Model(inputs = target_model.input, outputs = discriminator_model(temp))\n",
    "    \n",
    "    combined_model.compile(loss='binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    \n",
    "#get training data\n",
    "\n",
    "    source_data_generator = data_generator(X_train, Y_train, 64)\n",
    "    target_data_generator = data_generator(X_test, Y_test, 64)\n",
    "    \n",
    "    \n",
    "    # pre_train\n",
    "    source_model.fit(X_train, Y_train, batch_size = 25, epochs = 30, \n",
    "                        verbose = 1, validation_data=(X_validate, Y_validate),\n",
    "                        callbacks=[checkpointer], class_weight = class_weights,shuffle=True)\n",
    "    \n",
    "    source_classifer_model = Model(inputs = source_model.get_layer('elu_2').output, outputs = source_model.output)\n",
    "    \n",
    "    loss_target = np.zeros(shape=len(discriminator_model.metrics_names))\n",
    "    loss_disc = np.zeros(shape=len(discriminator_model.metrics_names))\n",
    "    \n",
    "    for epoch in range(1000):\n",
    "        \n",
    "        # Train discriminator\n",
    "        for _ in range(disc):\n",
    "            \n",
    "            xs, ys = next(source_data_generator)\n",
    "            \n",
    "            xt, yt = next(target_data_generator)\n",
    "            \n",
    "            ys = to_categorical(np.ones(len(ys)),num_classes=2)\n",
    "                        \n",
    "            yt = to_categorical(np.zeros(len(yt)),num_classes=2)\n",
    "            \n",
    "            source_feature = source_feature_model.predict(xs)\n",
    "            \n",
    "            target_feature = target_feature_model.predict(xt)\n",
    "            \n",
    "            disc_x = np.concatenate((source_feature, target_feature))\n",
    "            \n",
    "            disc_y = np.concatenate((ys, yt))\n",
    "            \n",
    "            loss_disc = np.add(discriminator_model.train_on_batch(disc_x, disc_y), loss_disc)\n",
    "                        \n",
    "        # Train target model\n",
    "        for _ in range(clf):\n",
    "            \n",
    "            xt, yt = next(target_data_generator)\n",
    "            \n",
    "            yt = to_categorical(np.ones(len(yt)),num_classes=2)\n",
    "            \n",
    "            xt_2, yt_2 = next(target_data_generator)\n",
    "            \n",
    "            yt_2 = to_categorical(np.ones(len(yt_2)),num_classes=2)\n",
    "            \n",
    "            combine_x = np.concatenate((xt, xt_2),axis = 0)\n",
    "            \n",
    "            combine_y = np.concatenate((yt, yt_2), axis = 0)\n",
    "            \n",
    "            loss_target= np.add(combined_model.train_on_batch(combine_x, combine_y), loss_target)\n",
    "        \n",
    "        if (epoch % 10) == 0:\n",
    "            print \"loss target\", loss_target/(10*clf)\n",
    "            print \"loss discrimnator\", loss_dis/(10*disc)\n",
    "\n",
    "            loss_target = np.zeros(shape=len(discriminator_model.metrics_names))\n",
    "            loss_dis= np.zeros(shape=len(discriminator_model.metrics_names))\n",
    "                    \n",
    "target_feature_model.save(\"targetModel/target_model.hdf5\")\n",
    "discriminator_model.save(\"discriminatorModel/discriminator_model.hdf5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-gpu]",
   "language": "python",
   "name": "conda-env-tf-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
